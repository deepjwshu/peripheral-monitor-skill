# å¤–è®¾æ–°å“ç›‘æ§æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ - æŠ€æœ¯è§„èŒƒä¸å·¥ä½œæµ

> **ç‰ˆæœ¬**: v1.1 (ä¸¥è°¨ä¿®è®¢ç‰ˆ) | **æ›´æ–°æ—¥æœŸ**: 2026-02-10
>
> æœ¬æ–‡æ¡£å®šä¹‰ç³»ç»Ÿçš„æ ¸å¿ƒå·¥ä½œæµã€æ•°æ®å£å¾„ã€é…ç½®è§„èŒƒï¼Œç”¨äºä»£ç å®¡æŸ¥ã€æ•…éšœæ’æŸ¥å’ŒåŠŸèƒ½æ‰©å±•ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
2. [æ ¸å¿ƒå·¥ä½œæµ](#æ ¸å¿ƒå·¥ä½œæµ)
3. [æ•°æ®å£å¾„ä¸ç¼ºå¤±å€¼å¤„ç†](#æ•°æ®å£å¾„ä¸ç¼ºå¤±å€¼å¤„ç†)
4. [å»é‡ä¸åˆå¹¶ç­–ç•¥](#å»é‡ä¸åˆå¹¶ç­–ç•¥)
5. [LLM é”™è¯¯å¤„ç†ä¸é™çº§](#llm-é”™è¯¯å¤„ç†ä¸é™çº§)
6. [æŠ¥å‘Šæ ¡éªŒè§„èŒƒ](#æŠ¥å‘Šæ ¡éªŒè§„èŒƒ)
7. [é…ç½®ä¸æ‰©å±•](#é…ç½®ä¸æ‰©å±•)
8. [éšç§ä¸å®‰å…¨](#éšç§ä¸å®‰å…¨)
9. [ç«™ç‚¹åˆè§„ä¸åçˆ¬ç­–ç•¥](#ç«™ç‚¹åˆè§„ä¸åçˆ¬ç­–ç•¥)

---

## ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¾“å…¥æ•°æ®æº                                 â”‚
â”‚  - Spider çˆ¬è™«: output/report_data_YYYY_MM.json                  â”‚
â”‚  - ç”¨æˆ·ä¸Šä¼ : ä»»æ„ JSON/Excel æ–‡ä»¶                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1: æ•°æ®æ¸…æ´—å±‚ (DataCleaner)                                â”‚
â”‚  - å…³é”®è¯ç™½åå•ç­›é€‰ (KEYWORDS)                                    â”‚
â”‚  - é»‘åå•è¿‡æ»¤ (BLACKLIST)                                         â”‚
â”‚  - æ™ºèƒ½å»é‡ (SequenceMatcher, é˜ˆå€¼=0.6)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2: LLM æ™ºèƒ½åˆ†æå±‚ (LLMExtractor V2)                        â”‚
â”‚  - å¹¶å‘å¤„ç† (ThreadPoolExecutor, batch_size=5)                   â”‚
â”‚  - å•æ¬¡è°ƒç”¨æå–: product_name + specs (Top 15) + analysis         â”‚
â”‚  - ä»·æ ¼æ ‡å‡†åŒ– (å¤–å¸è½¬æ¢ + æ­£åˆ™æå–)                                â”‚
â”‚  - å®Œæ•´æ€§éªŒè¯ (å¿…éœ€å­—æ®µæ£€æŸ¥)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 3: äº§å“åˆå¹¶å±‚ (ProductMerger)                              â”‚
â”‚  - åç§°å½’ä¸€åŒ– (normalize_product_name)                           â”‚
â”‚  - ç›¸ä¼¼åº¦è®¡ç®— (SequenceMatcher, é˜ˆå€¼=0.85)                        â”‚
â”‚  - é˜²è¯¯åˆå¹¶ç¡¬è§„åˆ™ (ç±»åˆ«å¿…é¡»ç›¸åŒ)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 4: æŠ¥å‘Šç”Ÿæˆå±‚ (HTMLReportGenerator)                        â”‚
â”‚  - å¸‚åœºåˆ†æ (å“ç±»å æ¯”ã€ä¼ æ„Ÿå™¨åˆ†å¸ƒã€ä»·æ ¼åŒºé—´)                        â”‚
â”‚  - æ–‡æœ¬ç”Ÿæˆ (æŠ€æœ¯è¶‹åŠ¿ã€ä»·æ ¼æ´å¯Ÿã€PM å¯ç¤º)                           â”‚
â”‚  - HTML æ„å»º (æ·±è‰²æå®¢é£ + äº¤äº’å¼å›¾è¡¨)                             â”‚
â”‚  - å››å‘ä¸€è‡´æ€§æ ¡éªŒ (DOM + æ•°æ®å£å¾„éªŒè¯)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¾“å‡ºäº¤ä»˜ç‰©                                 â”‚
â”‚  - monthly_report_YYYY_MM.html (ä¸»æŠ¥å‘Š)                           â”‚
â”‚  - monthly_report_YYYY_MM.html.bak (å¤‡ä»½)                         â”‚
â”‚  - processed_products.json (ç»“æ„åŒ–æ•°æ®)                           â”‚
â”‚  - logs/failed_items_*.json (å¤±è´¥é¡¹æ—¥å¿—)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒå·¥ä½œæµ

### ä¸»å…¥å£: `etl_pipeline.py::main()`

```python
def main(template_mode="pm_deep", input_file=None, target_year=None, target_month=None):
    """
    å‚æ•°:
        template_mode: "pm_deep" (é»˜è®¤) | "simple"
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º output/report_data_YYYY_MM.json
        target_year: ç›®æ ‡å¹´ä»½ï¼Œé»˜è®¤ä¸º TARGET_YEAR
        target_month: ç›®æ ‡æœˆä»½ï¼Œé»˜è®¤ä¸º TARGET_MONTH

    è¿”å›:
        None (ç”Ÿæˆ HTML æŠ¥å‘Šå’Œ JSON æ•°æ®)
    """
```

### æ•°æ®æµè½¬ç¤ºä¾‹ (2026-01 è¿è¡Œå®ä¾‹)

> **æ³¨æ„**: ä»¥ä¸‹æ•°å­—ä¸ºæŸæ¬¡å®é™…è¿è¡Œç¤ºä¾‹ï¼Œå®é™…æ•°å€¼ä¼šæ ¹æ®è¾“å…¥æ•°æ®åŠ¨æ€å˜åŒ–ã€‚

```
[åŸå§‹è¾“å…¥] 64 æ¡æ–‡ç« è®°å½•
    â†“ å…³é”®è¯ç­›é€‰ (åŒ…å« KEYWORDS ä»»ä¸€å…³é”®è¯)
[é˜¶æ®µ1è¾“å‡º] 60 æ¡ (ç§»é™¤ 4 æ¡æ— å…³å†…å®¹)
    â†“ é»‘åå•è¿‡æ»¤ (æ’é™¤åŒ…å« BLACKLIST å…³é”®è¯)
[é˜¶æ®µ2è¾“å‡º] 58 æ¡ (ç§»é™¤ 2 æ¡é…ä»¶è®°å½•)
    â†“ æ™ºèƒ½å»é‡ (SequenceMatcher > 0.6)
[é˜¶æ®µ3è¾“å‡º] 37 æ¬¾äº§å“ (åˆå¹¶ 21 æ¡é‡å¤è®°å½•)
    â†“ LLM åˆ†æ (batch_size=5, å¤±è´¥é‡è¯•3æ¬¡)
[é˜¶æ®µ4è¾“å‡º] 31 æ¬¾æœ‰æ•ˆäº§å“ (6 æ¬¾å›  completeness < 60% è¢«ä¸¢å¼ƒ)
    â†“ ä¼˜å…ˆçº§æ’åº + äº§å“åˆå¹¶ (SequenceMatcher > 0.85)
[é˜¶æ®µ5è¾“å‡º] 24 æ¬¾æœ€ç»ˆäº§å“ (åˆå¹¶ 7 æ¬¾é‡å¤äº§å“)
    â†“ æŠ¥å‘Šç”Ÿæˆ + å››å‘ä¸€è‡´æ€§æ ¡éªŒ
[æœ€ç»ˆäº¤ä»˜] HTML æŠ¥å‘Š (197 KB) + JSON æ•°æ®
```

**å˜é‡å£å¾„è¯´æ˜**:
- `åŸå§‹è¾“å…¥æ•°`: è¯»å–çš„ JSON/Excel æ–‡ä»¶è®°å½•æ€»æ•°
- `å…³é”®è¯ç­›é€‰å`: åŒ…å« KEYWORDS åˆ—è¡¨ä¸­ä»»æ„å…³é”®è¯çš„è®°å½•æ•°
- `é»‘åå•è¿‡æ»¤å`: æ’é™¤åŒ…å« BLACKLIST å…³é”®è¯çš„è®°å½•æ•°
- `æ™ºèƒ½å»é‡å`: åŸºäº SequenceMatcher > 0.6 åˆå¹¶åçš„äº§å“æ•°
- `LLM åˆ†æå`: é€šè¿‡å®Œæ•´æ€§éªŒè¯çš„äº§å“æ•° (completeness >= 60%)
- `ä¼˜å…ˆçº§æ’åºå`: æŒ‰ `_priority_score` é™åºæ’åˆ—çš„äº§å“æ•°
- `äº§å“åˆå¹¶å`: åŸºäº SequenceMatcher > 0.85 åˆå¹¶åçš„æœ€ç»ˆäº§å“æ•°

---

## æ•°æ®å£å¾„ä¸ç¼ºå¤±å€¼å¤„ç†

### ç¼ºå¤±å€¼ Taxonomy (ä¸¥è°¨æ ‡å‡†)

| æ ‡è®°å€¼ | å«ä¹‰ | äº§ç”Ÿåœºæ™¯ | ç¤ºä¾‹ | å›¾è¡¨æ¡¶ | Coverage | æç¤ºæ–‡æ¡ˆ |
|--------|------|----------|------|--------|----------|----------|
| **æœªæåŠ** | åŸæ–‡å®Œå…¨æ²¡æœ‰è¯¥ä¿¡æ¯ | æ–‡ç« æœªæåŠè¯¥å‚æ•° | "å‘å¸ƒæ—¥æœŸæœªå®š" | âŒ ä¸è®¡å…¥æ¡¶ | âŒ ä¸è®¡å…¥åˆ†å­ | "åŸæ–‡æœªæåŠ" |
| **æœªå…¬å¼€** | åŸæ–‡æ˜ç¡®è¯´æ˜æœªå…¬å¸ƒ | å‚å•†æœªå…¬å¼€ä»·æ ¼/å‚æ•° | "ä»·æ ¼æš‚æœªå…¬å¸ƒ" | âœ… "æœªå…¬å¼€"æ¡¶ | âŒ ä¸è®¡å…¥åˆ†å­ | "å‚å•†æœªå…¬å¼€" |
| **æå–å¤±è´¥** | LLM æœªèƒ½æˆåŠŸæå– | åŸæ–‡å¯èƒ½æœ‰ä½†æŠ½å–å¤±è´¥ | JSON parse å¤±è´¥ | âŒ ä¸è®¡å…¥æ¡¶ | âŒ ä¸è®¡å…¥åˆ†å­ | "æå–å¤±è´¥" |
| **æ— æ³•åˆ¤æ–­** | LLM æ— æ³•æ¨æ–­æ¥æº | ä¿¡æ¯æ¨¡ç³Šæˆ–çŸ›ç›¾ | "éƒ¨åˆ†å‹å·æœªçŸ¥" | âŒ ä¸è®¡å…¥æ¡¶ | âŒ ä¸è®¡å…¥åˆ†å­ | "æ— æ³•åˆ¤æ–­" |
| **å¾…å®æµ‹** | æ–°å“æœªä¸Šå¸‚éœ€å®æµ‹ | æ¦‚å¿µ/é¢„å‘å¸ƒäº§å“ | "å‚æ•°å¾…åç»­å®æµ‹" | âœ… "å¾…å®æµ‹"æ¡¶ | âŒ ä¸è®¡å…¥åˆ†å­ | "å¾…å®˜æ–¹å®æµ‹" |
| **æ¦‚å¿µäº§å“** | ä»…æ¦‚å¿µ/æ¸²æŸ“å›¾ | æ— å®ç‰©å‚æ•° | "ä»…å‘å¸ƒæ¸²æŸ“å›¾" | âœ… "æ¦‚å¿µ"æ¡¶ | âŒ ä¸è®¡å…¥åˆ†å­ | "æ¦‚å¿µäº§å“" |
| **æœªçŸ¥** (åºŸå¼ƒ) | ~~LLM æ— æ³•æ¨æ–­~~ | ~~å·²æ›¿æ¢ä¸º"æå–å¤±è´¥"~~ | - | - | - | - |

### å­—æ®µæœ‰æ•ˆæ€§åˆ¤æ–­

```python
# æœ‰æ•ˆå€¼åˆ¤æ–­ (etl_pipeline.py:4289)
INVALID_VALUES = ['æœªçŸ¥', 'æœªå…¬å¼€', 'N/A', '', 'null', 'none', 'unknown']

# âš ï¸ é‡è¦åŒºåˆ†:
# 1. "æœªå…¬å¼€" â†’ æœ‰æ•ˆå€¼ (å•ç‹¬ç»Ÿè®¡ä¸ºç‹¬ç«‹æ¡¶)
# 2. "æœªæåŠ" / "æå–å¤±è´¥" / "æ— æ³•åˆ¤æ–­" â†’ æ— æ•ˆå€¼ (ä¸è®¡å…¥ coverage)
# 3. "å¾…å®æµ‹" â†’ æœ‰æ•ˆå€¼ (å•ç‹¬ç»Ÿè®¡ä¸ºç‹¬ç«‹æ¡¶ï¼Œä½†ä¸è®¡å…¥ coverage)
# 4. "æ¦‚å¿µäº§å“" â†’ äº§å“çº§åˆ«æ ‡è®° (innovation_tags)

# åˆ¤æ–­é€»è¾‘:
def is_valid_value(value: str) -> bool:
    """åˆ¤æ–­å­—æ®µå€¼æ˜¯å¦æœ‰æ•ˆ"""
    if not value or not str(value).strip():
        return False
    value_str = str(value).strip()
    if value_str in ['æœªçŸ¥', 'unknown', 'null', 'none', 'N/A', '']:
        return False
    # "æœªå…¬å¼€"ã€"å¾…å®æµ‹" ç­‰æè¿°æ€§å€¼è¢«è§†ä¸ºæœ‰æ•ˆ
    return True
```

### ä»·æ ¼å¤„ç†è§„åˆ™

#### 1. ä»·æ ¼æå–ä¼˜å…ˆçº§

```python
# ä¼˜å…ˆçº§ (ä»é«˜åˆ°ä½):
# 1. specs.product_pricing (LLM æå–çš„ç»“æ„åŒ–ä»·æ ¼)
# 2. release_price (LLM æå–çš„ä»·æ ¼å­—æ®µ)
# 3. æ­£åˆ™è¡¨è¾¾å¼ä»åŸæ–‡æå– (å›é€€æœºåˆ¶)

# æå–é€»è¾‘:
price = (
    product.get('specs', {}).get('product_pricing', '') or
    product.get('release_price', '') or
    extract_price_from_content(product['content_text'])  # æ­£åˆ™å›é€€
)
```

#### 2. ä»·æ ¼æ ‡å‡†åŒ– (`_standardize_price()`)

```python
# æ”¯æŒæ ¼å¼:
# - "299å…ƒ" â†’ "299å…ƒ"
# - "99-299å…ƒ" â†’ "299å…ƒ" (å–ä¸Šé™ï¼Œå½“å‰å®ç°)
# - "$49.99" â†’ "360å…ƒ" (æ±‡ç‡è½¬æ¢: 1 USD = 7.2 CNY)
# - "æœªå…¬å¼€" â†’ "ä»·æ ¼æœªå…¬å¼€"
# - "å¾…å®š" â†’ "ä»·æ ¼æœªå…¬å¼€"

# âš ï¸ å‡ä»·è®¡ç®—å£å¾„:
# å½“å‰: å–ä»·æ ¼åŒºé—´ä¸Šé™ç”¨äºè®¡ç®—
# ç¤ºä¾‹: "99-299å…ƒ" æŒ‰ 299 å…ƒè®¡å…¥å‡ä»·
# å»ºè®®: æœªæ¥åº”è§£æä¸º {min: 99, max: 299, mid: 199, currency: 'CNY'}
#       å‡ä»·ä½¿ç”¨ mid å€¼æ›´å‡†ç¡®

# æ±‡ç‡é…ç½® (2025 å¹´å‚è€ƒ):
EXCHANGE_RATES = {
    'USD': 7.2,    # 1ç¾å…ƒ = 7.2äººæ°‘å¸
    '$': 7.2,
    'JPY': 0.048,  # 1æ—¥å…ƒ = 0.048äººæ°‘å¸
    'EUR': 7.8,    # 1æ¬§å…ƒ = 7.8äººæ°‘å¸
}
```

#### 3. ä»·æ ¼åŒºé—´åˆ’åˆ† (å›¾è¡¨ç»Ÿè®¡)

```python
PRICE_RANGES = {
    '0-199å…ƒ':    0 <= price_num < 200,
    '200-499å…ƒ':  200 <= price_num < 500,
    '500-999å…ƒ':  500 <= price_num < 1000,
    '1000å…ƒ+':    price_num >= 1000,
    'æœªå…¬å¼€':     æ— æ³•è§£æä»·æ ¼æˆ–æ ‡è®°ä¸º"æœªå…¬å¼€"/"å¾…å®š"
}

# âš ï¸ "æœªå…¬å¼€"æ¡¶åŒ…å«:
# 1. åŸæ–‡æ˜ç¡®"æœªå…¬å¼€"
# 2. æ— æ³•è§£æä¸ºæ•°å­—çš„ä»»ä½•å€¼
# 3. ç©ºå­—ç¬¦ä¸²æˆ– None
```

#### 4. å‡ä»·è®¡ç®—å£å¾„

```python
# analyze_pricing_insights() å‡ä»·è®¡ç®—:
def calculate_average_price(products: List[Dict]) -> int:
    """
    å‡ä»· = æœ‰æ•ˆä»·æ ¼æ€»å’Œ / æœ‰æ•ˆä»·æ ¼æ•°é‡

    æœ‰æ•ˆä»·æ ¼: å¯è§£æä¸ºæ•°å­—çš„ price å€¼
    æœªå…¬å¼€: ä¸è®¡å…¥åˆ†æ¯å’Œåˆ†å­

    ç¤ºä¾‹:
        5æ¬¾é¼ æ ‡: [99, 199, 299, æœªå…¬å¼€, æœªå…¬å¼€]
        å‡ä»· = (99 + 199 + 299) / 3 = 199 å…ƒ

        å«åŒºé—´: [99-299, 199-399, æœªå…¬å¼€]
        å½“å‰: (299 + 399) / 2 = 349 å…ƒ (å–ä¸Šé™)
        å»ºè®®: (199 + 299) / 2 = 249 å…ƒ (å–ä¸­å€¼)
    """
    valid_prices = []
    for product in products:
        price = extract_price_number(product)
        if price is not None:  # å¯è§£æä¸ºæ•°å­—
            valid_prices.append(price)

    if not valid_prices:
        return 0

    return sum(valid_prices) // len(valid_prices)
```

### Coverage å®šä¹‰ (ä¸¥è°¨å£å¾„)

**Coverage = æœ‰æœ‰æ•ˆå€¼çš„äº§å“æ•° / ç»Ÿè®¡å¯¹è±¡äº§å“æ€»æ•°**

#### æ ¸å¿ƒæ¦‚å¿µåŒºåˆ†

| æ¦‚å¿µ | å®šä¹‰ | ä½¿ç”¨åœºæ™¯ | è®¡ç®—å…¬å¼ |
|------|------|----------|----------|
| **Coverage** | æŸå­—æ®µæœ‰æœ‰æ•ˆå€¼çš„å æ¯” | å®è§‚æ•°æ®è´¨é‡è¯„ä¼° | æœ‰å€¼äº§å“æ•° / æ€»äº§å“æ•° |
| **Completeness** | å•äº§å“ specs å­—æ®µå®Œæ•´åº¦ | äº§å“çº§è´¨é‡è¿‡æ»¤ | éç©ºå­—æ®µæ•° / Schema æ€»å­—æ®µæ•° |

#### ä¼ æ„Ÿå™¨è¦†ç›–ç‡

```python
# ç»Ÿè®¡å¯¹è±¡: æ‰€æœ‰é¼ æ ‡äº§å“ (self.mice)
# æœ‰æ•ˆå€¼åˆ¤æ–­: specs.sensor_solution éç©ºä¸”ä¸åœ¨ INVALID_VALUES ä¸­
#            æ’é™¤: "æœªæåŠ"ã€"æå–å¤±è´¥"ã€"æ— æ³•åˆ¤æ–­"
#            åŒ…å«: "æœªå…¬å¼€" (å•ç‹¬ç»Ÿè®¡æ¡¶)

def calculate_sensor_coverage(products: List[Dict]) -> float:
    """
    ä¼ æ„Ÿå™¨è¦†ç›–ç‡è®¡ç®—

    ç»Ÿè®¡å¯¹è±¡: æ‰€æœ‰ category == 'é¼ æ ‡' çš„äº§å“
    æœ‰æ•ˆå€¼: specs.sensor_solution æ»¡è¶³ä»¥ä¸‹æ¡ä»¶:
        1. éç©ºå­—ç¬¦ä¸²
        2. ä¸åœ¨ INVALID_VALUES ä¸­ (æ’é™¤ 'æœªçŸ¥', 'unknown', '')
        3. ä¸ä¸º 'æœªæåŠ', 'æå–å¤±è´¥', 'æ— æ³•åˆ¤æ–­'

    ç¤ºä¾‹:
        8 æ¬¾é¼ æ ‡:
        - PAW3395, PAW3950, Hero (æœ‰æ•ˆ) â†’ 3
        - æœªæåŠ, æœªæåŠ, æœªæåŠ, æœªæåŠ, æœªæåŠ (æ— æ•ˆ) â†’ 5
        coverage = 3 / 8 = 37.5%
    """
    mice = [p for p in products if p.get('category') == 'é¼ æ ‡']
    total = len(mice)

    if total == 0:
        return 0.0

    valid_count = 0
    invalid_markers = ['æœªæåŠ', 'æå–å¤±è´¥', 'æ— æ³•åˆ¤æ–­', '']

    for mouse in mice:
        sensor = mouse.get('specs', {}).get('sensor_solution', '')
        if sensor and sensor not in invalid_markers:
            valid_count += 1

    return valid_count / total
```

#### ä»·æ ¼è¦†ç›–ç‡

```python
# ç»Ÿè®¡å¯¹è±¡: æ‰€æœ‰äº§å“ (self.products)
# æœ‰æ•ˆå€¼åˆ¤æ–­: product_pricing æˆ– release_price å¯è§£æä¸ºæ•°å­—
#            æ’é™¤: "æœªå…¬å¼€"ã€"å¾…å®š"

def calculate_price_coverage(products: List[Dict]) -> float:
    """
    ä»·æ ¼è¦†ç›–ç‡è®¡ç®—

    ç»Ÿè®¡å¯¹è±¡: æ‰€æœ‰äº§å“ (é¼ æ ‡ + é”®ç›˜ + å…¶ä»–)
    æœ‰æ•ˆå€¼: æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€:
        1. specs.product_pricing å¯è§£æä¸ºæ•°å­—
        2. release_price å¯è§£æä¸ºæ•°å­—

    æ— æ•ˆå€¼:
        - "æœªå…¬å¼€"ã€"å¾…å®š"ã€"æš‚æ— ä»·æ ¼"
        - æ— æ³•è§£æçš„ä»»ä½•å­—ç¬¦ä¸²

    ç¤ºä¾‹:
        24 æ¬¾äº§å“:
        - 15 æ¬¾æœ‰ä»·æ ¼ (æœ‰æ•ˆ)
        - 9 æ¬¾æœªå…¬å¼€ (æ— æ•ˆ)
        coverage = 15 / 24 = 62.5%
    """
    total = len(products)

    if total == 0:
        return 0.0

    valid_count = 0
    for product in products:
        price = (
            product.get('specs', {}).get('product_pricing', '') or
            product.get('release_price', '')
        )
        # å°è¯•è§£æä¸ºæ•°å­—
        price_num = extract_price_number(price)
        if price_num is not None:
            valid_count += 1

    return valid_count / total
```

#### Coverage å‘Šè­¦é˜ˆå€¼

```python
# è­¦å‘Šè§„åˆ™:
def check_coverage_quality(coverage_stats: Dict) -> List[str]:
    """
    Coverage è´¨é‡æ£€æŸ¥

    å‘Šè­¦é˜ˆå€¼:
        - coverage == 0: å®Œå…¨ç¼ºå¤±ï¼Œä¸¥é‡å‘Šè­¦
        - coverage < 0.2: æä½ï¼Œå‘Šè­¦
        - coverage < 0.5: è¾ƒä½ï¼Œè­¦å‘Š
        - coverage >= 0.5: å¯æ¥å—
    """
    warnings = []

    sensor_cov = coverage_stats['sensor']['coverage']
    price_cov = coverage_stats['price']['coverage']

    if sensor_cov == 0:
        warnings.append("âš ï¸  ä¼ æ„Ÿå™¨æ•°æ®å®Œå…¨ç¼ºå¤± (0%)ï¼Œæ‰€æœ‰é¼ æ ‡äº§å“çš„ä¼ æ„Ÿå™¨ä¿¡æ¯æ— æ•ˆ")
    elif sensor_cov < 0.2:
        warnings.append(f"âš ï¸  ä¼ æ„Ÿå™¨è¦†ç›–ç‡æä½ ({sensor_cov:.1%}) < 20%")
    elif sensor_cov < 0.5:
        warnings.append(f"âš ï¸  ä¼ æ„Ÿå™¨è¦†ç›–ç‡è¾ƒä½ ({sensor_cov:.1%}) < 50%ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡")

    if price_cov == 0:
        warnings.append("âš ï¸  ä»·æ ¼æ•°æ®å®Œå…¨ç¼ºå¤± (0%)ï¼Œæ‰€æœ‰äº§å“çš„ä»·æ ¼ä¿¡æ¯æ— æ•ˆ")
    elif price_cov < 0.2:
        warnings.append(f"âš ï¸  ä»·æ ¼è¦†ç›–ç‡æä½ ({price_cov:.1%}) < 20%")
    elif price_cov < 0.5:
        warnings.append(f"âš ï¸  ä»·æ ¼è¦†ç›–ç‡è¾ƒä½ ({price_cov:.1%}) < 50%ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡")

    return warnings
```

---

## å»é‡ä¸åˆå¹¶ç­–ç•¥

### ä¸¤çº§å»é‡æ¶æ„

#### Level 1: æ–‡ç« çº§åˆ«å»é‡ (DataCleaner.smart_deduplicate)

**ç›®çš„**: åˆå¹¶å¤šç¯‡å…³äºåŒä¸€äº§å“çš„ä¸åŒæŠ¥é“

**æµç¨‹**:
```python
# 1. æŒ‰å‘å¸ƒæ—¶é—´æ’åº (æœ€æ–°çš„åœ¨å‰)
df = df.sort_values('publish_date', ascending=False)

# 2. é€æ¡æ¯”å¯¹ç›¸ä¼¼åº¦
for i, row_i in df.iterrows():
    for j, row_j in df.iterrows():
        if j <= i: continue

        similarity = SequenceMatcher(None, row_i['title'], row_j['title']).ratio()

        # é˜ˆå€¼: 0.6 (è§ä¸‹æ–¹"é˜ˆå€¼è¯´æ˜")
        if similarity > 0.6:
            # åˆå¹¶è®°å½•
            merged_product['records'].extend([row_i, row_j])
            merged_product['sources'] = list(set([row_i['source'], row_j['source']]))
            merged_product['combined_content'] = join_content(row_i, row_j)
```

**è¾“å‡º**:
```python
{
    'product_name': 'æ ‡å‡†åŒ–äº§å“å…¨å',
    'records': [...],        # åŸå§‹æ–‡ç« åˆ—è¡¨
    'sources': ['inå¤–è®¾', 'å¤–è®¾å¤©ä¸‹'],
    'images': ['url1', 'url2'],
    'combined_content': 'åˆå¹¶åçš„æ­£æ–‡...'
}
```

#### Level 2: äº§å“çº§åˆ«å»é‡ (ProductMerger.merge_products)

**ç›®çš„**: åˆå¹¶ LLM åˆ†æåçš„é‡å¤äº§å“

**æµç¨‹**:
```python
# 1. æŒ‰ç±»åˆ«åˆ†ç»„
by_category = groupby(products, key='category')

# 2. åç§°å½’ä¸€åŒ–
def normalize_product_name(name: str) -> str:
    # ç§»é™¤ç©ºæ ¼ (å¤„ç† "G304 X" vs "G304X")
    normalized = re.sub(r'\s+', '', name)
    # ç»Ÿä¸€å¤§å°å†™
    normalized = normalized.lower()
    # ç§»é™¤å“ç‰Œåç¼€ (é¿å… "Logitech G304" vs "G304" æ— æ³•åŒ¹é…)
    normalized = re.sub(r'(lightspeed|wireless|gaming|rgb|pro|ultra|max|heroc|ç‰ˆ|æ— çº¿|æœ‰çº¿)', '', normalized)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    normalized = re.sub(r'[^a-z0-9\u4e00-\u9fff]', '', normalized)
    return normalized

# 3. ç›¸ä¼¼åº¦è®¡ç®—
similarity = SequenceMatcher(
    None,
    normalize_product_name(name1),
    normalize_product_name(name2)
).ratio()

# 4. é˜²è¯¯åˆå¹¶ç¡¬è§„åˆ™
if category_i != category_j:
    return False  # ä¸åŒç±»åˆ«ï¼Œä¸åˆå¹¶ (ç¡¬è§„åˆ™)

# 5. åˆå¹¶åˆ¤æ–­
if similarity > 0.85:  # é˜ˆå€¼: 0.85 (è§ä¸‹æ–¹"é˜ˆå€¼è¯´æ˜")
    # åˆå¹¶ specs: å–å¹¶é›† (éç©ºå­—æ®µä¼˜å…ˆ)
    # åˆå¹¶ analysis: ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬
    # åˆå¹¶ä»·æ ¼: ä¼˜å…ˆä½¿ç”¨æœ‰ä»·æ ¼çš„
    # è®°å½• merged_from_count
```

### é˜ˆå€¼é…ç½®ä¸è°ƒå‚æŒ‡å—

```python
# etl_pipeline.py (å»ºè®®æå–ä¸ºé…ç½®)
DEDUPLICATE_CONFIG = {
    'article_level': {
        'similarity_threshold': 0.6,      # æ–‡ç« æ ‡é¢˜ç›¸ä¼¼åº¦
        'sort_by': 'publish_date',         # æ’åºå­—æ®µ
        'ascending': False                 # é™åº (æœ€æ–°çš„åœ¨å‰)
    },
    'product_level': {
        'similarity_threshold': 0.85,     # äº§å“åç§°ç›¸ä¼¼åº¦
        'category_must_match': True,       # ç±»åˆ«å¿…é¡»ç›¸åŒ
        'priority_field': '_priority_score' # ä¼˜å…ˆçº§å­—æ®µ
    }
}
```

#### é˜ˆå€¼è®¾è®¡è¯´æ˜

| é˜ˆå€¼ | è®¾å®šå€¼ | åŸå›  | å…¸å‹æ¡ˆä¾‹ |
|------|--------|------|----------|
| **æ–‡ç« çº§ 0.6** | 0.6 | å®½æ¾ç­–ç•¥ï¼Œé¿å…é—æ¼åŒä¸€äº§å“çš„ä¸åŒæŠ¥é“ | âœ… "ç½—æŠ€G Pro X Superlight" vs "ç½—æŠ€GPWæ–°å“" â†’ 0.65 â†’ åˆå¹¶<br>âŒ "ç½—æŠ€G304" vs "ç½—æŠ€G304 X" â†’ 0.55 â†’ ä¸åˆå¹¶ |
| **äº§å“çº§ 0.85** | 0.85 | ä¸¥æ ¼ç­–ç•¥ï¼Œé¿å…è¯¯åˆå¹¶ä¸åŒå‹å· | âœ… "VGNèœ»èœ“F1 Max" vs "VGNèœ»èœ“F1 MOBA" â†’ 0.88 â†’ åˆå¹¶<br>âŒ "VGNèœ»èœ“F1" vs "VGNèœ»èœ“F2" â†’ 0.75 â†’ ä¸åˆå¹¶ |

#### è°ƒå‚æŒ‡å—

**ä½•æ—¶è°ƒæ•´æ–‡ç« çº§é˜ˆå€¼ (0.6)**:

| åœºæ™¯ | å»ºè®®å€¼ | ç†ç”± |
|------|--------|------|
| ç½‘ç«™æ ‡é¢˜æ ¼å¼ç»Ÿä¸€ | 0.7 - 0.8 | æé«˜ç²¾ç¡®åº¦ï¼Œå‡å°‘è¯¯åˆå¹¶ |
| ç½‘ç«™æ ‡é¢˜æ ¼å¼æ··ä¹± | 0.5 - 0.6 | é™ä½é˜ˆå€¼ï¼Œé¿å…é—æ¼ |
| æ ‡é¢˜åŒ…å«å¤§é‡ä¿®é¥°è¯ | 0.5 | éœ€è¦æ›´å®½æ¾çš„åŒ¹é… |

**ä½•æ—¶è°ƒæ•´äº§å“çº§é˜ˆå€¼ (0.85)**:

| åœºæ™¯ | å»ºè®®å€¼ | ç†ç”± |
|------|--------|------|
| åŒç³»åˆ—äº§å“å¤š (å¦‚ VGN èœ»èœ“ F1/F2/F3) | 0.90 - 0.95 | é¿å…è¯¯åˆå¹¶ä¸åŒå‹å· |
| äº§å“åç§°é«˜åº¦æ ‡å‡†åŒ– | 0.85 | ä¿æŒå½“å‰å€¼ |
| å­˜åœ¨å¤§é‡ OEM/ODM åŒè´´ç‰Œ | 0.80 | å…è®¸åˆå¹¶åŒæ¨¡å…·äº§å“ |

**è°ƒå‚éªŒè¯æ–¹æ³•**:

```bash
# 1. è¿è¡Œå»é‡
python etl_pipeline.py --month 2026-01

# 2. æ£€æŸ¥ merged_from_count å­—æ®µ
cat output/processed_products.json | jq '.[].merged_from_count'

# 3. äººå·¥æŠ½æŸ¥åˆå¹¶æ˜¯å¦æ­£ç¡®
# å¦‚æœ merged_from_count > 3ï¼Œå¯èƒ½é˜ˆå€¼è¿‡ä½ï¼Œéœ€è¦è°ƒé«˜
# å¦‚æœå­˜åœ¨æ˜æ˜¾é‡å¤äº§å“æœªåˆå¹¶ï¼Œéœ€è¦è°ƒä½é˜ˆå€¼
```

### é˜²è¯¯åˆå¹¶ç¡¬è§„åˆ™

| è§„åˆ™ | è¯´æ˜ | å½“å‰å®ç° | ä¼˜å…ˆçº§ |
|------|------|----------|--------|
| **ç±»åˆ«ä¸€è‡´æ€§** | ä¸åŒç±»åˆ«çš„äº§å“ä¸åˆå¹¶ | âœ… å·²å®ç° | P0 |
| **å“ç‰Œå·®å¼‚ä¿æŠ¤** | ä¸åŒå“ç‰Œçš„ç›¸ä¼¼äº§å“ä¸åˆå¹¶ | âŒ å¾…å®ç° | P1 |
| **å‹å·åŒºåˆ†** | åŒç³»åˆ—ä¸åŒå‹å·ä¸åˆå¹¶ (å¦‚ F1 vs F2) | âš ï¸ éƒ¨åˆ†å®ç° (ä¾èµ–é˜ˆå€¼) | P1 |
| **ä»·æ ¼å·®å¼‚å‘Šè­¦** | ä»·æ ¼å·®å¼‚ >50% å‘å‡ºå‘Šè­¦ | âŒ å¾…å®ç° | P2 |

**å»ºè®®å¢å¼º**:

```python
# å“ç‰Œå·®å¼‚ä¿æŠ¤ (P1)
BRANDS = ['ç½—æŠ€', 'é›·è›‡', 'VGN', 'ATK', 'ZAOKEN', ...]

def extract_brand(product_name: str) -> Optional[str]:
    """æå–äº§å“å“ç‰Œ"""
    for brand in BRANDS:
        if brand in product_name:
            return brand
    return None

# åœ¨åˆå¹¶å‰æ£€æŸ¥
if extract_brand(name1) != extract_brand(name2):
    return False  # ä¸åŒå“ç‰Œï¼Œä¸åˆå¹¶
```

---

## LLM é”™è¯¯å¤„ç†ä¸é™çº§

### é JSON å“åº”å¤„ç†

```python
# æµç¨‹ (extract_product_info_v2):
def extract_product_info_v2(self, context: Dict) -> Dict:
    try:
        # 1. å°è¯•ç›´æ¥è§£æ JSON
        extracted_data = json.loads(llm_response)
    except JSONDecodeError:
        # 2. æ¸…ç† markdown ä»£ç å—æ ‡è®°
        if llm_response.startswith('```'):
            llm_response = llm_response.split('```')[1]
            if llm_response.startswith('json'):
                llm_response = llm_response[4:]
        # 3. å†æ¬¡å°è¯•è§£æ
        try:
            extracted_data = json.loads(llm_response)
        except JSONDecodeError:
            # 4. è®°å½•å¤±è´¥å¹¶è¿”å›é™çº§æ•°æ®
            return {
                'error': 'JSON parse failed',
                'raw_response': llm_response[:500],  # ä¿ç•™å‰ 500 å­—ç¬¦ç”¨äºè°ƒè¯•
                'product_name': context.get('title', 'Unknown')
            }
```

### Schema æ ¡éªŒ

```python
# å¿…éœ€å­—æ®µæ£€æŸ¥
REQUIRED_FIELDS = ['product_name', 'category', 'specs', 'analysis']

for field in REQUIRED_FIELDS:
    if field not in extracted_data:
        raise ValueError(f"Missing required field: {field}")

# specs å­å­—æ®µå®Œæ•´æ€§ (completeness)
if category == 'é¼ æ ‡':
    critical_fields = MOUSE_SCHEMA.keys()
elif category == 'é”®ç›˜':
    critical_fields = KEYBOARD_SCHEMA.keys()

complete_fields = sum(1 for f in critical_fields if extracted_data['specs'].get(f))
completeness = complete_fields / len(critical_fields)

if completeness < 0.6:
    # æ ‡è®°ä¸º dropped (æ³¨æ„: è¿™é‡Œç”¨ completenessï¼Œä¸æ˜¯ coverage)
    return {
        'dropped': True,
        'reason': f'Completeness {completeness:.1%} < 60%',
        'completeness': completeness
    }
```

### å¤±è´¥é¡¹é™çº§ç­–ç•¥

```python
# ç­–ç•¥ 1: é‡è¯•æœºåˆ¶
max_retries = 3
for attempt in range(max_retries):
    try:
        result = call_llm_api(prompt)
        break
    except Exception as e:
        if attempt == max_retries - 1:
            # è®°å½•å¤±è´¥é¡¹
            failed_items.append({
                'index': idx,
                'reason': str(e),
                'product_name': product['product_name'],
                'attempt': max_retries
            })
            dropped_count += 1

# ç­–ç•¥ 2: ä¿ç•™åŸå§‹æ•°æ®
# å³ä½¿ LLM åˆ†æå¤±è´¥ï¼Œä¿ç•™åŸå§‹æ–‡ç« å†…å®¹åˆ° processed_products.json
# ä¾¿äºåç»­äººå·¥è¡¥å½•æˆ–äºŒæ¬¡å¤„ç†
```

### é”™è¯¯æ—¥å¿—æ ¼å¼

```json
// logs/failed_items_20260210_172540.json
[
  {
    "index": 3,
    "reason": "APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: 429 Client Error: Too Many Requests",
    "product_name": "æŸäº§å“å",
    "attempt": 3,
    "timestamp": "2026-02-10T17:25:40"
  },
  {
    "index": 7,
    "reason": "Completeness 40% < 60%",
    "product_name": "å¦ä¸€äº§å“å",
    "completeness": 0.4,
    "completeness_detail": {
      "total_fields": 15,
      "complete_fields": 6,
      "category": "é¼ æ ‡"
    }
  }
]
```

---

## æŠ¥å‘Šæ ¡éªŒè§„èŒƒ

### DOM ç»“æ„æ ¡éªŒ (å½“å‰å®ç°)

```python
def _validate_html_structure(html_content: str) -> bool:
    """
    æ ¡éªŒå¿…éœ€ DOM å…ƒç´ çš„å­˜åœ¨æ€§

    å¿…éœ€ç»„ä»¶:
        - nav-bar: å¯¼èˆªæ 
        - searchInput: æœç´¢æ¡†
        - product-overview: äº§å“æ¦‚è§ˆæ¨¡å— (å·¦æ )
        - product-specs: ç¡¬æ ¸å‚æ•°æ¨¡å— (ä¸­æ )
        - product-analysis: PM æ·±åº¦æ´å¯Ÿæ¨¡å— (å³æ )
        - PM æ·±åº¦æ´å¯Ÿ: PM æ ‡é¢˜æ–‡æœ¬
    """
    required_components = {
        'nav-bar': r'<nav[^>]*id="nav-bar"',
        'searchInput': r'<input[^>]*id="searchInput"',
        'product-overview': r'<div[^>]*id="product-overview"',
        'product-specs': r'<div[^>]*id="product-specs"',
        'product-analysis': r'<div[^>]*id="product-analysis"',
        'PM æ·±åº¦æ´å¯Ÿ': r'PM\s*æ·±åº¦æ´å¯Ÿ'
    }

    for name, pattern in required_components.items():
        if not re.search(pattern, html_content):
            raise AssertionError(f"[FAIL] ç¼ºå°‘å¿…éœ€ç»„ä»¶: {name}")

    return True
```

### å››å‘ä¸€è‡´æ€§æ ¡éªŒ (ä¸¥è°¨å®ç°)

```python
def validate_four_way_consistency(generator: HTMLReportGenerator) -> Dict[str, Any]:
    """
    å››å‘ä¸€è‡´æ€§æ ¡éªŒ:

    1. nav_counts (å¯¼èˆªæ ç»Ÿè®¡) == section_counts (åˆ†åŒºæ ‡é¢˜æ•°)
    2. section_counts == chart_counts (å“ç±»å›¾è¡¨æ•°æ®)
    3. chart_counts == card_counts (äº§å“å¡ç‰‡æ•°)
    4. æ¯ä¸ªå›¾è¡¨: sum(data) == ç»Ÿè®¡å¯¹è±¡æ•°é‡

    ä¸ä¸€è‡´æ—¶:
        - æŠ›å‡º AssertionError (ä¸æ˜¯ warning)
        - è¾“å‡ºå®šä½ä¿¡æ¯ (ç¼ºå¤±/å¤šç®—çš„äº§å“ id åˆ—è¡¨)
        - é˜»æ­¢æŠ¥å‘Šç”Ÿæˆ
    """
    issues = []

    # æå–å„å‘æ•°æ®
    nav_stats = _extract_nav_stats(generator.html)
    section_stats = _extract_section_stats(generator.html)
    chart_data = generator.market_analyzer.get_chart_data()
    card_count = len(generator.products)

    # 1. å¯¼èˆªæ  vs åˆ†åŒºæ ‡é¢˜
    if nav_stats != section_stats:
        diff = _diff_dict(nav_stats, section_stats)
        issues.append(f"å¯¼èˆªæ ä¸åˆ†åŒºæ ‡é¢˜ä¸ä¸€è‡´: {diff}")

    # 2. åˆ†åŒºæ ‡é¢˜ vs å›¾è¡¨æ•°æ®
    category_chart = {
        cat: count
        for cat, count in zip(chart_data['category_data']['labels'], chart_data['category_data']['data'])
    }

    if section_stats != category_chart:
        diff = _diff_dict(section_stats, category_chart)
        issues.append(f"åˆ†åŒºæ ‡é¢˜ä¸å›¾è¡¨æ•°æ®ä¸ä¸€è‡´: {diff}")

    # 3. å›¾è¡¨æ•°æ® vs äº§å“å¡ç‰‡
    total_chart = sum(chart_data['category_data']['data'])
    if total_chart != card_count:
        # å®šä½å·®å¼‚äº§å“
        card_ids = set(p.get('id', p['product_name']) for p in generator.products)
        chart_ids = _extract_chart_product_ids(generator.html)
        missing = card_ids - chart_ids
        extra = chart_ids - card_ids
        issues.append(
            f"å›¾è¡¨æ€»æ•°({total_chart}) != å¡ç‰‡æ•°({card_count})\n"
            f"  ç¼ºå¤±äº§å“: {list(missing)[:5]}\n"
            f"  å¤šç®—äº§å“: {list(extra)[:5]}"
        )

    # 4. å„å›¾è¡¨å†…éƒ¨ä¸€è‡´æ€§
    # ä¼ æ„Ÿå™¨å›¾è¡¨: sum(data) == len(mice)
    sensor_chart_sum = sum(chart_data['sensor_dist'].values())
    mice_count = len(generator.mice)
    if sensor_chart_sum != mice_count:
        issues.append(f"ä¼ æ„Ÿå™¨å›¾è¡¨æ€»å’Œ({sensor_chart_sum}) != é¼ æ ‡æ•°({mice_count})")

    # ä»·æ ¼å›¾è¡¨: sum(data) == len(products)
    price_chart_sum = sum(chart_data['price_ranges'].values())
    if price_chart_sum != card_count:
        issues.append(f"ä»·æ ¼å›¾è¡¨æ€»å’Œ({price_chart_sum}) != äº§å“æ•°({card_count})")

    # åˆ¤æ–­æ˜¯å¦é€šè¿‡
    if issues:
        error_msg = "[FAIL] å››å‘ä¸€è‡´æ€§æ ¡éªŒå¤±è´¥:\n" + "\n".join(issues)
        raise AssertionError(error_msg)

    return {
        'passed': True,
        'nav_stats': nav_stats,
        'section_stats': section_stats,
        'chart_data': chart_data,
        'card_count': card_count
    }


def _diff_dict(d1: Dict, d2: Dict) -> Dict:
    """å¯¹æ¯”ä¸¤ä¸ªå­—å…¸çš„å·®å¼‚"""
    diff = {}
    all_keys = set(d1.keys()) | set(d2.keys())
    for key in all_keys:
        v1 = d1.get(key, 0)
        v2 = d2.get(key, 0)
        if v1 != v2:
            diff[key] = {'d1': v1, 'd2': v2}
    return diff


def _extract_nav_stats(html: str) -> Dict:
    """ä»å¯¼èˆªæ æå–ç»Ÿè®¡ä¿¡æ¯"""
    # æ­£åˆ™åŒ¹é…: <a href="#mice">é¼ æ ‡ (8)</a>
    pattern = r'<a href="#(\w+)">\w+\s*\((\d+)\)</a>'
    matches = re.findall(pattern, html)
    return {cat: int(count) for cat, count in matches}


def _extract_section_stats(html: str) -> Dict:
    """ä»åˆ†åŒºæ ‡é¢˜æå–ç»Ÿè®¡ä¿¡æ¯"""
    # æ­£åˆ™åŒ¹é…: <h2>é¼ æ ‡æ–°å“ (8æ¬¾)</h2>
    pattern = r'<h2>\w+æ–°å“\s*\((\d+)æ¬¾\)</h2>'
    # æ ¹æ®ç±»åˆ«åˆ†ç»„
    sections = re.findall(r'<h2>(\w+)æ–°å“\s*\((\d+)æ¬¾\)</h2>', html)
    return {cat: int(count) for cat, count in sections}


def _extract_chart_product_ids(html: str) -> Set:
    """ä»å›¾è¡¨ä¸­æå–äº§å“ id"""
    # æ­£åˆ™åŒ¹é…äº§å“å¡ç‰‡çš„ id å±æ€§
    pattern = r'<div[^>]*id="product-card-([^"]+)"'
    return set(re.findall(pattern, html))
```

### å†…å®¹è´¨é‡æ ¡éªŒ (P2 å¯é€‰)

```python
def validate_content_quality(generator: HTMLReportGenerator) -> Dict[str, Any]:
    """
    å†…å®¹è´¨é‡æ ¡éªŒ:
        - å¤±è´¥é¡¹å æ¯”é˜ˆå€¼
        - specs å®Œæ•´æ€§åˆ†å¸ƒ (completeness)
        - coverage ç»Ÿè®¡
    """
    total = len(generator.products)
    failed = load_failed_items_count()

    if failed / total > 0.2:
        issues.append(f"âš ï¸  å¤±è´¥é¡¹å æ¯”è¿‡é«˜: {failed}/{total} ({failed/total:.1%})")

    # completeness åˆ†å¸ƒ (å•äº§å“çº§åˆ«)
    completeness_distribution = {
        'high (>80%)': 0,
        'medium (50-80%)': 0,
        'low (<50%)': 0
    }

    for product in generator.products:
        completeness = calculate_specs_completeness(product['specs'])
        if completeness > 0.8:
            completeness_distribution['high (>80%)'] += 1
        elif completeness >= 0.5:
            completeness_distribution['medium (50-80%)'] += 1
        else:
            completeness_distribution['low (<50%)'] += 1

    # coverage ç»Ÿè®¡ (å®è§‚æ•°æ®)
    coverage = generator.market_analyzer.get_coverage_stats()

    return {
        'completeness_distribution': completeness_distribution,
        'coverage': coverage,
        'failed_ratio': failed / total
    }
```

---

## é…ç½®ä¸æ‰©å±•

### ç™½åå•/é»‘åå•é…ç½®

```python
# etl_pipeline.py (lines 115-126)

# ç™½åå•å…³é”®è¯ (åŒ…å«ä»»ä¸€å…³é”®è¯å³ä¿ç•™)
KEYWORDS = [
    "é¼ æ ‡", "é”®ç›˜", "é”®é¼ ", "å®¢åˆ¶åŒ–", "è½´ä½“",
    "æœºæ¢°é”®ç›˜", "ç£è½´",
    # "æ‰‹æŸ„"  # é»˜è®¤æ³¨é‡Šï¼Œå¯é€šè¿‡ INCLUDE_GAMEPAD=True å¯ç”¨
]

# é»‘åå•å…³é”®è¯ (åŒ…å«ä»»ä¸€å…³é”®è¯å³æ’é™¤)
BLACKLIST = [
    # é…ä»¶ç±»
    "é¼ æ ‡å«", "æ¡Œå«", "çº¿æ", "æ”¶çº³åŒ…", "è€³æœºæ¶",
    "ç†çº¿å™¨", "è„šè´´", "é˜²æ»‘è´´", "æ‰‹æ‰˜", "è…•æ‰˜",
    # éŸ³é¢‘ç±»
    "è€³æœº", "éŸ³ç®±", "æ‰¬å£°å™¨", "éº¦å…‹é£",
    # å…¶ä»–å¤–è®¾
    "æ‘„åƒå¤´", "æ˜¾ç¤ºå™¨", "æ”¯æ¶", "hub", "é›†çº¿å™¨", "æ‰©å±•å"
]

# æ‰‹æŸ„å¼€å…³ (é…ç½®åŒ–)
INCLUDE_GAMEPAD = False  # è®¾ä¸º True ä»¥åŒ…å«æ¸¸æˆæ‰‹æŸ„
if INCLUDE_GAMEPAD:
    KEYWORDS.append("æ‰‹æŸ„")
```

### Schema å®šä¹‰

#### é¼ æ ‡ Top 15 å‚æ•° (MOUSE_SCHEMA)

```python
MOUSE_SCHEMA = {
    # å¿…å¡«å­—æ®µ (ç”¨äº completeness è¯„åˆ†)
    'product_pricing': 'äº§å“ä¸å®šä»·',
    'mold_lineage': 'æ¨¡å…·è¡€ç»Ÿ',
    'weight_center': 'é‡é‡ä¸é‡å¿ƒ',
    'sensor_solution': 'ä¼ æ„Ÿå™¨æ–¹æ¡ˆ',  # ç”¨äºå›¾è¡¨ç»Ÿè®¡
    'polling_rate': 'å›æŠ¥ç‡é…ç½®',
    # å¯é€‰å­—æ®µ
    'mcu_chip': 'ä¸»æ§èŠ¯ç‰‡',
    'end_to_end_latency': 'å…¨é“¾è·¯å»¶è¿Ÿ',
    'switch_features': 'å¾®åŠ¨ç‰¹æ€§',
    'scroll_encoder': 'æ»šè½®ç¼–ç å™¨',
    'coating_process': 'æ¶‚å±‚å·¥è‰º',
    'high_refresh_battery': 'é«˜åˆ·ç»­èˆª',
    'structure_quality': 'ç»“æ„åšå·¥',
    'feet_config': 'è„šè´´é…ç½®',
    'wireless_interference': 'æ— çº¿æŠ—å¹²æ‰°',
    'driver_experience': 'é©±åŠ¨ä½“éªŒ'
}
```

#### é”®ç›˜ Top 15 å‚æ•° (KEYBOARD_SCHEMA)

```python
KEYBOARD_SCHEMA = {
    # å¿…å¡«å­—æ®µ
    'product_layout': 'äº§å“ä¸é…åˆ—',
    'structure_form': 'ç»“æ„å½¢å¼',
    'switch_details': 'è½´ä½“è¯¦è§£',  # ç”¨äºç£è½´ç»Ÿè®¡
    'tech_route': 'æŠ€æœ¯è·¯çº¿',
    # å¯é€‰å­—æ®µ
    'rt_params': 'RTå‚æ•°',
    'sound_dampening': 'å£°éŸ³åŒ…å¡«å……',
    'measured_latency': 'å®æµ‹å»¶è¿Ÿ',
    'keycap_craftsmanship': 'é”®å¸½å·¥è‰º',
    'bigkey_tuning': 'å¤§é”®è°ƒæ ¡',
    'pcb_features': 'PCBç‰¹æ€§',
    'case_craftsmanship': 'å¤–å£³å·¥è‰º',
    'front_height': 'å‰é«˜æ•°æ®',
    'battery_efficiency': 'ç”µæ± æ•ˆç‡',
    'connection_storage': 'è¿æ¥ä¸æ”¶çº³',
    'software_support': 'è½¯ä½“æ”¯æŒ'
}
```

### å¹¶å‘é…ç½®

```python
# etl_pipeline.py (lines 4787-4789)
CONCURRENT_CONFIG = {
    'batch_size': 5,          # å¹¶å‘æ•°
    'max_retries': 3,         # å¤±è´¥é‡è¯•æ¬¡æ•°
    'timeout': 120,           # å•æ¬¡è¯·æ±‚è¶…æ—¶ (ç§’)
    'temperature': 0.1,       # LLM æ¸©åº¦å‚æ•°
    'max_tokens': 3500        # LLM æœ€å¤§ tokens
}

# æ€§èƒ½ä¼˜åŒ–å»ºè®®:
# - API æ€§èƒ½å¼ºæ—¶: batch_size=10
# - å†…å­˜æœ‰é™æ—¶: batch_size=3
# - é«˜ç¨³å®šæ€§: batch_size=1 (ä¸²è¡Œ)
```

### LLM API é…ç½®

```python
# .env æ–‡ä»¶é…ç½®
LLM_API_KEY=sk-your-api-key-here
LLM_API_BASE=http://192.168.0.250:7777  # æˆ– https://api.deepseek.com/v1
LLM_MODEL=xdeepseekv3                   # æˆ– deepseek-chat

# æ”¯æŒ OpenAI å…¼å®¹æ ¼å¼çš„ä»»ä½• API
```

---

## éšç§ä¸å®‰å…¨

### æ•°æ®éšç§ä¿æŠ¤

| æ•°æ®ç±»å‹ | å­˜å‚¨ä½ç½® | è®¿é—®æ§åˆ¶ | ä¿ç•™æœŸé™ | å¤–å‘é£é™© |
|----------|----------|----------|----------|----------|
| åŸå§‹æ–‡ç« æ•°æ® | `output/report_data_YYYY_MM.json` | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | æ°¸ä¹… | âŒ æ—  |
| LLM åˆ†æç»“æœ | `output/processed_products.json` | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | æ°¸ä¹… | âŒ æ—  |
| å¤±è´¥é¡¹æ—¥å¿— | `logs/failed_items_*.json` | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | æ°¸ä¹… | âš ï¸ å¯èƒ½åŒ…å« snippet |
| å¤‡ä»½æŠ¥å‘Š | `output/*.html.bak` | æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ | æ°¸ä¹… | âŒ æ—  |

**éšç§åŸåˆ™**:
- æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨æœ¬åœ°ï¼Œä¸ä¸Šä¼ è‡³äº‘ç«¯ (é™¤ LLM API è°ƒç”¨å¤–)
- **LLM API è°ƒç”¨**: ä¼šå‘ `LLM_API_BASE` å‘é€æ–‡ç« æ­£æ–‡ (`content_text`) ç”¨äºåˆ†æ
- æŠ¥å‘Šä¸ºé™æ€ HTMLï¼Œæ— ç”¨æˆ·è¿½è¸ªä»£ç 
- æ—¥å¿—ä¸åŒ…å« API å¯†é’¥ (è§ä¸‹æ–‡)

### API å¯†é’¥ç®¡ç†

```bash
# .env æ–‡ä»¶ (ä¸æäº¤åˆ° Git)
LLM_API_KEY=sk-your-api-key-here

# ç¯å¢ƒå˜é‡ (ä¼˜å…ˆçº§æ›´é«˜)
export LLM_API_KEY=sk-your-api-key-here
```

**å®‰å…¨è¦æ±‚**:
- âœ… ä½¿ç”¨ `.gitignore` æ’é™¤ `.env` æ–‡ä»¶
- âœ… å®šæœŸè½®æ¢ API å¯†é’¥
- âœ… ä½¿ç”¨å†…éƒ¨ API ç«¯ç‚¹ (å¦‚ http://192.168.0.250:7777)
- âœ… æ—¥å¿—ä¸­ä¸è½å¯†é’¥ (ä»»ä½• print/logger ä¸å¾—è¾“å‡º LLM_API_KEY)
- âŒ ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API å¯†é’¥
- âŒ ä¸è¦å°† `.env.example` ä¸­çš„çœŸå®å¯†é’¥æäº¤åˆ° Git

**æ—¥å¿—è„±æ•ç¤ºä¾‹**:

```python
import os

def log_llm_config(config: Dict):
    """å®‰å…¨åœ°è®°å½• LLM é…ç½®"""
    api_key = config.get('api_key', '')
    # è„±æ•: åªæ˜¾ç¤ºå‰ 4 å’Œå 4 å­—ç¬¦
    masked_key = f"{api_key[:4]}...{api_key[-4:]}" if len(api_key) > 8 else "***"

    log_msg = f"LLM Config: model={config.get('model')}, base={config.get('base_url')}, key={masked_key}"
    print(log_msg)  # å®‰å…¨è¾“å‡º
```

### æ•æ„Ÿå†…å®¹è¿‡æ»¤

```python
# å½“å‰æœªå®ç°ï¼Œå»ºè®®æ–°å¢ (P1):
SENSITIVE_PATTERNS = [
    (r'\d{15,18}', '[èº«ä»½è¯å·]'),      # èº«ä»½è¯å·
    (r'1[3-9]\d{9}', '[æ‰‹æœºå·]'),     # æ‰‹æœºå·
    (r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[é‚®ç®±]'),  # é‚®ç®±
    (r'password["\']?\s*[:=]\s*["\']?[^\s"\']+', '[å¯†ç ]'),  # å¯†ç å­—æ®µ
]

def sanitize_content(content: str) -> str:
    """
    è¿‡æ»¤æ•æ„Ÿå†…å®¹

    ç”¨é€”:
        1. LLM å‘é€å‰è„±æ• (é˜²æ­¢æ•æ„Ÿä¿¡æ¯å‘é€åˆ° API)
        2. HTML æŠ¥å‘Šç”Ÿæˆæ—¶è„±æ• (é˜²æ­¢æŠ¥å‘Šæ³„éœ²æ•æ„Ÿä¿¡æ¯)
    """
    for pattern, replacement in SENSITIVE_PATTERNS:
        content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
    return content

# ä½¿ç”¨ç¤ºä¾‹:
# LLM å‘é€å‰
sanitized_content = sanitize_content(product['content_text'])
llm_response = call_llm_api(sanitized_content)
```

### LLM API æ•°æ®å¤–å‘å£°æ˜

```
âš ï¸ é‡è¦éšç§æç¤º:

æœ¬ç³»ç»Ÿä½¿ç”¨ LLM API è¿›è¡Œäº§å“åˆ†æï¼Œä»¥ä¸‹æ•°æ®ä¼šå‘é€åˆ° LLM_API_BASE:

  - æ–‡ç« æ ‡é¢˜ (title)
  - æ–‡ç« æ­£æ–‡ (content_text)
  - æ–‡ç« æ¥æº (source)

ä¸ä¼šå‘é€:
  - ç”¨æˆ·èº«ä»½ä¿¡æ¯
  - API å¯†é’¥
  - æœ¬åœ°æ–‡ä»¶è·¯å¾„

å¦‚éœ€å¯ç”¨æ•æ„Ÿå†…å®¹è¿‡æ»¤ï¼Œè¯·åœ¨ config.py ä¸­è®¾ç½®:
    SANITIZE_CONTENT = True
```

---

## ç«™ç‚¹åˆè§„ä¸åçˆ¬ç­–ç•¥

### çˆ¬è™«è¡Œä¸ºè§„èŒƒ

```python
# spider.py é…ç½®
SPIDER_CONFIG = {
    'max_pages': 20,           # æ¯ä¸ªç«™ç‚¹æœ€å¤§çˆ¬å–é¡µæ•°
    'request_delay': 2,        # è¯·æ±‚é—´éš” (ç§’) - ä¸ .env ä¿æŒä¸€è‡´
    'delay_jitter': 1.0,       # éšæœºæŠ–åŠ¨èŒƒå›´ (Â±1ç§’)
    'timeout': 30,             # è¯·æ±‚è¶…æ—¶ (ç§’)
    'user_agent': 'Mozilla/5.0 ...',  # å›ºå®šçœŸå®æµè§ˆå™¨ UA
    'respect_robots_txt': True # éµå®ˆ robots.txt (å»ºè®®æ–°å¢)
}
```

### é€Ÿç‡é™åˆ¶

```python
# .env é…ç½® (ä¸ SPIDER_CONFIG ä¿æŒä¸€è‡´)
SPIDER_DELAY_SECONDS=2       # åŸºç¡€å»¶è¿Ÿ (æ¨è 2-4 ç§’)
SPIDER_DELAY_JITTER=1.0      # éšæœºæŠ–åŠ¨ (Â±1 ç§’)

# å½“å‰å®ç° (éšå¼):
# - å•çº¿ç¨‹é¡ºåºçˆ¬å–
# - æ— æ˜¾å¼ sleep (ä¾èµ–ç½‘ç»œå»¶è¿Ÿ)

# å»ºè®®å®ç° (P0 ä¼˜å…ˆçº§):
import time
import random

request_delay = float(os.getenv('SPIDER_DELAY_SECONDS', 2))
delay_jitter = float(os.getenv('SPIDER_DELAY_JITTER', 1.0))

for url in urls:
    response = fetch(url)
    time.sleep(request_delay + random.uniform(-delay_jitter, delay_jitter))
```

**æŠ–åŠ¨é—´éš”æ¨èå€¼**:

| Profile | åŸºç¡€å»¶è¿Ÿ | æŠ–åŠ¨èŒƒå›´ | é€‚ç”¨åœºæ™¯ |
|---------|----------|----------|----------|
| Conservative | 4s | Â±2s (2-6s) | ä¸¥æ ¼åçˆ¬ç«™ç‚¹ |
| Standard | 2s | Â±1s (1-3s) | é»˜è®¤æ¨è |
| Aggressive | 1s | Â±0.5s (0.5-1.5s) | æœ¬åœ°æµ‹è¯• |

### User-Agent ç­–ç•¥

```python
# å½“å‰: å›ºå®šçœŸå® UA
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

# å»ºè®®é…ç½®åŒ– (P1 ä¼˜å…ˆçº§):
# .env é…ç½®
SPIDER_UA_ROTATE=false  # false=å›ºå®šçœŸå®UA, true=è½®æ¢UA
SPIDER_UA_LIST='Mozilla/5.0 ...;Chrome/119.0.0.0 ...;Safari/17.0 ...'

USER_AGENTS = os.getenv('SPIDER_UA_LIST', USER_AGENT).split(';')

def get_user_agent() -> str:
    """è·å– User-Agent"""
    if os.getenv('SPIDER_UA_ROTATE', 'false').lower() == 'true':
        return random.choice(USER_AGENTS)
    return USER_AGENTS[0]  # é»˜è®¤å›ºå®šç¬¬ä¸€ä¸ª

# âš ï¸ é‡è¦: UA "è½®æ¢"è¡¨è¿°ä¸å‡†ç¡®ï¼Œå®é™…åº”ä¸º"å¯é…ç½®/çœŸå®å›ºå®š"
# - å›ºå®šçœŸå® UA: æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
# - è½®æ¢çœŸå® UA: åœ¨å¤šä¸ªçœŸå® UA é—´åˆ‡æ¢ï¼Œé™ä½å°ç¦é£é™©
# - ä¸å»ºè®®: ä½¿ç”¨ä¼ªé€ /æ˜æ˜¾é”™è¯¯çš„ UA
```

### robots.txt éµå®ˆ (P0 ä¼˜å…ˆçº§)

```python
# å½“å‰æœªå®ç°ï¼Œå»ºè®®æ–°å¢:
from urllib.robotparser import RobotFileParser

ROBOT_PARSER_CACHE = {}  # ç¼“å­˜ robots.txt è§£æç»“æœ

def can_fetch(url: str, user_agent: str = '*') -> bool:
    """
    æ£€æŸ¥æ˜¯å¦å…è®¸çˆ¬å–è¯¥ URL

    éµå®ˆ robots.txt è§„åˆ™:
        - User-agent: * é€‚ç”¨äºæ‰€æœ‰çˆ¬è™«
        - Disallow: /admin  ç¦æ­¢çˆ¬å– /admin è·¯å¾„
        - Allow: /public  å…è®¸çˆ¬å– /public è·¯å¾„
    """
    from urllib.parse import urlparse

    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"

    if base_url not in ROBOT_PARSER_CACHE:
        rp = RobotFileParser()
        rp.set_url(f"{base_url}/robots.txt")
        try:
            rp.read()
            ROBOT_PARSER_CACHE[base_url] = rp
        except Exception as e:
            print(f"[WARNING] æ— æ³•è¯»å– robots.txt: {base_url}, {e}")
            return True  # æ— æ³•è¯»å–æ—¶é»˜è®¤å…è®¸

    rp = ROBOT_PARSER_CACHE[base_url]
    return rp.can_fetch(user_agent, parsed.path)

# ä½¿ç”¨ç¤ºä¾‹:
for url in urls:
    if not can_fetch(url):
        print(f"[SKIP] robots.txt ç¦æ­¢çˆ¬å–: {url}")
        continue
    response = fetch(url)
```

### ç«™ç‚¹å‹å¥½ç­–ç•¥

| ç­–ç•¥ | å½“å‰å®ç° | å»ºè®®ä¼˜å…ˆçº§ | æ¨èé…ç½® |
|------|----------|------------|----------|
| è¯·æ±‚é—´éš” | éšå¼ (ç½‘ç»œå»¶è¿Ÿ) | P0: æ˜¾å¼ sleep | 2s Â± 1s jitter |
| User-Agent | å›ºå®š | P1: å¯é…ç½® | å›ºå®šçœŸå® UA |
| robots.txt | âŒ æœªéµå®ˆ | P0: æ·»åŠ æ£€æŸ¥ | éµå®ˆ Disallow è§„åˆ™ |
| å¹¶å‘é™åˆ¶ | å•çº¿ç¨‹ | P2: å¯é…ç½®å¹¶å‘ | é»˜è®¤å•çº¿ç¨‹ |
| é”™è¯¯é‡è¯• | æ—  | P1: æŒ‡æ•°é€€é¿ | 2^attempt ç§’å»¶è¿Ÿ |
| æœ€å¤§é¡µæ•° | 20 é¡µ/ç«™ç‚¹ | P1: å¯é…ç½® | 20-50 é¡µ |

### æ³•å¾‹åˆè§„

**æ•°æ®ä½¿ç”¨è§„èŒƒ**:
- âœ… ä»…ç”¨äºä¸ªäºº/å…¬å¸å†…éƒ¨åˆ†æ
- âœ… ä¸ç”¨äºå•†ä¸šç«äº‰æˆ–æ¶æ„ç”¨é€”
- âœ… æ³¨æ˜æ•°æ®æ¥æº (å¦‚ "æ•°æ®æ¥æº: inå¤–è®¾")
- âŒ ä¸è½¬å”®æˆ–å…¬å¼€å‘å¸ƒçˆ¬å–çš„æ•°æ®
- âŒ ä¸ç”¨äºè®­ç»ƒç«å“æ¨¡å‹

**å»ºè®®æ·»åŠ å…è´£å£°æ˜**:
```html
<!-- åœ¨ HTML æŠ¥å‘Šåº•éƒ¨æ·»åŠ  -->
<footer>
  <p>æ•°æ®æ¥æº: inå¤–è®¾ã€å¤–è®¾å¤©ä¸‹ | ä»…ä¾›å†…éƒ¨å‚è€ƒï¼Œä¸¥ç¦å¤–ä¼ </p>
  <p>æœ¬æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½å­˜åœ¨è¯¯å·®ï¼Œè¯·ä»¥å®˜æ–¹ä¿¡æ¯ä¸ºå‡†</p>
  <p>ç”Ÿæˆæ—¶é—´: 2026-02-10 | ç³»ç»Ÿç‰ˆæœ¬: v1.0</p>
</footer>
```

---

## æ•…éšœæ’æŸ¥æ‰‹å†Œ

### å¸¸è§é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| `KeyError: 'æœªçŸ¥'` | ä¼ æ„Ÿå™¨åˆ†å¸ƒå­—å…¸ä¸­'æœªçŸ¥'æ¡¶ä¸å­˜åœ¨ | âœ… å·²ä¿®å¤: ä½¿ç”¨ `.get('æœªçŸ¥', 0)` |
| `AssertionError: ç»Ÿè®¡å£å¾„ä¸ä¸€è‡´` | å¯¼èˆªæ /å›¾è¡¨/å¡ç‰‡æ•°é‡ä¸ä¸€è‡´ | æ£€æŸ¥å››å‘ä¸€è‡´æ€§æ ¡éªŒ |
| `JSONDecodeError` | LLM è¿”å›é JSON æ ¼å¼ | æ£€æŸ¥ `extract_product_info_v2()` è§£æé€»è¾‘ |
| `429 Too Many Requests` | API é€Ÿç‡é™åˆ¶ | é™ä½ `batch_size` æˆ–å¢åŠ  `SPIDER_DELAY_SECONDS` |
| `UnicodeEncodeError` | Windows æ§åˆ¶å°ç¼–ç é—®é¢˜ | åŠŸèƒ½æ­£å¸¸ï¼Œå¯å¿½ç•¥æˆ–ä½¿ç”¨ UTF-8 ç»ˆç«¯ |
| `Coverage < 50%` | æ•°æ®è´¨é‡å·®ï¼Œå¤§é‡å­—æ®µæœªæå– | æ£€æŸ¥ LLM Prompt å’Œè¾“å…¥æ•°æ®è´¨é‡ |

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è°ƒè¯•è¾“å‡º
DEBUG = True
LOG_LEVEL = 'DEBUG'

# æŸ¥çœ‹è°ƒè¯•ä¿¡æ¯:
# [DEBUG-CHART] ä¼ æ„Ÿå™¨åˆ†å¸ƒç»“æœ: {...}
# [DEBUG-LLM] specséç©ºå­—æ®µ: [...]
```

### æ—¥å¿—æ–‡ä»¶

```
logs/
â”œâ”€â”€ failed_items_20260210_172540.json  # å¤±è´¥é¡¹è¯¦æƒ…
â””â”€â”€ spider.log                         # çˆ¬è™«æ—¥å¿— (å¯é€‰)
```

---

## å˜æ›´ç‚¹æ‘˜è¦ (v1.0 â†’ v1.1)

### 1. ç¼ºå¤±å€¼ Taxonomy ä¿®æ­£

```diff
@@ v1.0 ç¼ºå¤±å€¼å®šä¹‰ @@
- | æœªçŸ¥ | LLM æ— æ³•æ¨æ–­çš„å‚æ•° (å·²åºŸå¼ƒ) | ç»Ÿä¸€æ›¿æ¢ä¸º"æœªæåŠ" |

@@ v1.1 ä¸¥è°¨å®šä¹‰ @@
+ | æ ‡è®°å€¼ | äº§ç”Ÿåœºæ™¯ | å›¾è¡¨æ¡¶ | Coverage |
+ |--------|----------|--------|----------|
+ | æœªæåŠ | åŸæ–‡å®Œå…¨æ²¡æœ‰è¯¥ä¿¡æ¯ | âŒ ä¸è®¡å…¥ | âŒ ä¸è®¡å…¥ |
+ | æœªå…¬å¼€ | åŸæ–‡æ˜ç¡®è¯´æ˜æœªå…¬å¸ƒ | âœ… ç‹¬ç«‹æ¡¶ | âŒ ä¸è®¡å…¥ |
+ | æå–å¤±è´¥ | LLM æœªèƒ½æˆåŠŸæå– | âŒ ä¸è®¡å…¥ | âŒ ä¸è®¡å…¥ |
+ | æ— æ³•åˆ¤æ–­ | LLM æ— æ³•æ¨æ–­æ¥æº | âŒ ä¸è®¡å…¥ | âŒ ä¸è®¡å…¥ |
+ | å¾…å®æµ‹ | æ–°å“æœªä¸Šå¸‚éœ€å®æµ‹ | âœ… ç‹¬ç«‹æ¡¶ | âŒ ä¸è®¡å…¥ |
+ | æ¦‚å¿µäº§å“ | ä»…æ¦‚å¿µ/æ¸²æŸ“å›¾ | âœ… ç‹¬ç«‹æ¡¶ | âŒ ä¸è®¡å…¥ |
+ | æœªçŸ¥ | å·²åºŸå¼ƒ â†’ æ›¿æ¢ä¸º"æå–å¤±è´¥" | - | - |
```

### 2. Coverage å…¬å¼æ”¹å†™

```diff
@@ v1.0 Coverage å®šä¹‰ @@
- Coverage = æœ‰æ•ˆå­—æ®µæ•°é‡ / æ€»æ ·æœ¬æ•°é‡

@@ v1.1 ä¸¥è°¨å£å¾„ @@
+ Coverage = æœ‰æœ‰æ•ˆå€¼çš„äº§å“æ•° / ç»Ÿè®¡å¯¹è±¡äº§å“æ€»æ•°
+
+ æ¦‚å¿µåŒºåˆ†:
+ - Coverage: æŸå­—æ®µæœ‰æœ‰æ•ˆå€¼çš„å æ¯” (å®è§‚æ•°æ®è´¨é‡)
+ - Completeness: å•äº§å“ specs å­—æ®µå®Œæ•´åº¦ (äº§å“çº§è´¨é‡)
+
+ é€å­—æ®µå®šä¹‰:
+ - ä¼ æ„Ÿå™¨è¦†ç›–ç‡: ç»Ÿè®¡å¯¹è±¡ = æ‰€æœ‰é¼ æ ‡äº§å“
+ - ä»·æ ¼è¦†ç›–ç‡: ç»Ÿè®¡å¯¹è±¡ = æ‰€æœ‰äº§å“ (é¼ æ ‡+é”®ç›˜+å…¶ä»–)
```

### 3. ä¸€è‡´æ€§æ ¡éªŒå¢å¼º

```diff
@@ v1.0 ä¸€è‡´æ€§æ ¡éªŒ @@
- ä¸‰å‘ä¸€è‡´æ€§æ ¡éªŒ (å»ºè®®):
- 1. å¯¼èˆªæ ç»Ÿè®¡ == äº§å“å¡ç‰‡
- 2. åˆ†åŒºæ ‡é¢˜ == å“ç±»å›¾è¡¨
- 3. å›¾è¡¨æ•°æ® == HTML å¡ç‰‡

@@ v1.1 å››å‘ä¸€è‡´æ€§ + å¤±è´¥å®šä½ @@
+ å››å‘ä¸€è‡´æ€§æ ¡éªŒ (ä¸¥è°¨å®ç°):
+ 1. nav_counts == section_counts
+ 2. section_counts == chart_counts
+ 3. chart_counts == card_counts
+ 4. æ¯ä¸ªå›¾è¡¨: sum(data) == ç»Ÿè®¡å¯¹è±¡æ•°é‡
+
+ ä¸ä¸€è‡´æ—¶:
+ - æŠ›å‡º AssertionError (FAIL è€Œé WARNING)
+ - è¾“å‡ºå®šä½ä¿¡æ¯ (ç¼ºå¤±/å¤šç®—çš„äº§å“ id åˆ—è¡¨)
+ - é˜»æ­¢æŠ¥å‘Šç”Ÿæˆ
```

### 4. ç«™ç‚¹åˆè§„è¡¨è¿°æ›´ä¸€è‡´

```diff
@@ v1.0 æŠ–åŠ¨é—´éš” @@
- request_delay = 2  # åŸºç¡€å»¶è¿Ÿ
- jitter = random.uniform(0.5, 1.5)  # éšæœºæŠ–åŠ¨

@@ v1.1 ä¸ .env ä¿æŒä¸€è‡´ @@
+ # .env é…ç½®
+ SPIDER_DELAY_SECONDS=2       # æ¨èå€¼: 2-4ç§’
+ SPIDER_DELAY_JITTER=1.0      # æŠ–åŠ¨èŒƒå›´: Â±1ç§’
+
+ request_delay = float(os.getenv('SPIDER_DELAY_SECONDS', 2))
+ jitter = float(os.getenv('SPIDER_DELAY_JITTER', 1.0))

@@ v1.0 UA è½®æ¢ @@
- User-Agent è½®æ¢

@@ v1.1 å‡†ç¡®è¡¨è¿° @@
+ User-Agent ç­–ç•¥:
+ - å›ºå®šçœŸå® UA (é»˜è®¤): æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
+ - å¯é…ç½®çœŸå® UA: åœ¨å¤šä¸ªçœŸå® UA é—´åˆ‡æ¢
+ - ä¸å»ºè®®ä½¿ç”¨ä¼ªé€ /é”™è¯¯ UA
```

### 5. éšç§ä¸å®‰å…¨è¡¨è¿°æ›´å‡†ç¡®

```diff
@@ v1.0 æ•°æ®éšç§ @@
- æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨æœ¬åœ°ï¼Œä¸ä¸Šä¼ è‡³äº‘ç«¯ (é™¤ LLM API è°ƒç”¨å¤–)

@@ v1.1 æ˜ç¡®å¤–å‘èŒƒå›´ +
+ LLM API è°ƒç”¨ä¼šå‘é€:
+ - æ–‡ç« æ ‡é¢˜ (title)
+ - æ–‡ç« æ­£æ–‡ (content_text)
+ - æ–‡ç« æ¥æº (source)
+
+ ä¸ä¼šå‘é€:
+ - ç”¨æˆ·èº«ä»½ä¿¡æ¯
+ - API å¯†é’¥
+ - æœ¬åœ°æ–‡ä»¶è·¯å¾„
+
+ è„±æ•å»ºè®®:
+ - æ—¥å¿—ä¸è½å¯†é’¥: ä½¿ç”¨è„±æ•æ ¼å¼ sk-xxx...xxxx
+ - æ•æ„Ÿå†…å®¹è¿‡æ»¤: èº«ä»½è¯å·/æ‰‹æœºå·/é‚®ç®±/å¯†ç 
```

### 6. å»é‡é˜ˆå€¼è¡¥å……è¯´æ˜

```diff
@@ v1.0 é˜ˆå€¼é…ç½® @@
- 'similarity_threshold': 0.6,  # æ–‡ç« æ ‡é¢˜ç›¸ä¼¼åº¦
- 'similarity_threshold': 0.85, # äº§å“åç§°ç›¸ä¼¼åº¦

@@ v1.1 ä¸ºä½•è¿™ä¹ˆè®¾ + å¦‚ä½•è°ƒå‚ +
+ | é˜ˆå€¼ | è®¾å®šå€¼ | åŸå›  | å…¸å‹æ¡ˆä¾‹ |
+ |------|--------|------|----------|
+ | æ–‡ç« çº§ 0.6 | 0.6 | å®½æ¾ç­–ç•¥ï¼Œé¿å…é—æ¼ | "ç½—æŠ€G Pro X" vs "ç½—æŠ€GPWæ–°å“" â†’ 0.65 â†’ åˆå¹¶ |
+ | äº§å“çº§ 0.85 | 0.85 | ä¸¥æ ¼ç­–ç•¥ï¼Œé¿å…è¯¯åˆå¹¶ | "VGNèœ»èœ“F1" vs "VGNèœ»èœ“F2" â†’ 0.75 â†’ ä¸åˆå¹¶ |
+
+ è°ƒå‚æŒ‡å—:
+ - ç½‘ç«™æ ‡é¢˜ç»Ÿä¸€: 0.7-0.8
+ - åŒç³»åˆ—äº§å“å¤š: 0.90-0.95
+ - è°ƒå‚éªŒè¯: æ£€æŸ¥ merged_from_count å­—æ®µ
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ GitHub Issueã€‚

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1 (ä¸¥è°¨ä¿®è®¢ç‰ˆ)
**æœ€åæ›´æ–°**: 2026-02-10
**ç»´æŠ¤è€…**: Claude Code Assistant
